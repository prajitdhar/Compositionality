{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join,getsize\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "import pickle as pkl\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=ResourceWarning)\n",
    "np.random.seed(seed=1991)\n",
    "import tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list=pd.read_csv(\"/data/dharp/compounding/datasets/words_list.txt\",header=None)\n",
    "word_list=word_list[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = pkl.load( open( \"mithril_files.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/dharp/compounding/datasets/entire_df_10_.h5',\n",
       " '/data/dharp/compounding/datasets/entire_df_1_.h5',\n",
       " '/data/dharp/compounding/datasets/entire_df_9_.h5',\n",
       " '/data/dharp/compounding/datasets/entire_df_11_.h5',\n",
       " '/data/dharp/compounding/datasets/entire_df_0_.h5',\n",
       " '/data/dharp/compounding/datasets/entire_df_6_.h5']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "br_to_us=pd.read_excel(\"Book.xlsx\")\n",
    "br_to_us_dict=dict(zip(br_to_us.UK.tolist(),br_to_us.US.tolist()))\n",
    "\n",
    "contextwords_df=pd.read_csv(\"contexts.csv\",sep=\"\\t\")\n",
    "contextwords=contextwords_df.context.tolist()\n",
    "\n",
    "\n",
    "adv_dict=dict(zip(['adv'],['r']))\n",
    "adv_replacement={'context_pos':adv_dict}\n",
    "spelling_replacement={'context':br_to_us_dict,'modifier':br_to_us_dict,'head':br_to_us_dict,'word':br_to_us_dict}\n",
    "\n",
    "\n",
    "pos_replacement={'pos':dict(zip([\"noun\",\"verb\",\"adj\"],['n','v','a']))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@da.as_gufunc(signature=\"(i)->()\", output_dtypes=str, vectorize=True)\n",
    "def lemma_maker(x, y):\n",
    "    return lemmatizer.lemmatize(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relemjoin(df,col_name,lemmatize=True):\n",
    "    new_col=col_name.split('_')[0]\n",
    "    new_col_pos=new_col[0]+\"_pos\"\n",
    "    df[new_col]=df[col_name].str.split('_', 1).str[0]\n",
    "    df[new_col_pos]=\"n\"\n",
    "    if lemmatize==True:\n",
    "        df[new_col]=np.vectorize(lemma_maker)(df[new_col], df[new_col_pos])\n",
    "    df[new_col]=df[new_col]+\"_n\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntactic_reducer(df,align,level=None):\n",
    "    if len(df) == 0:\n",
    "        print(\"Am here\")\n",
    "        return df\n",
    "    if align==\"right\":\n",
    "        if level==\"word\":\n",
    "            #t1=time.time()\n",
    "            df=df.loc[df.fivegram_pos.str.match(r'^[-a-z]+_noun\\s+[-a-z]+_.+\\s+[-a-z]+_.+\\s+[-a-z]+_.+\\s+[-a-z]+_.+$')]\n",
    "            if len(df) == 0:\n",
    "                return df\n",
    "            \n",
    "            df['word_pos'],df['r1_pos'],df['r2_pos'],df['r3_pos'],_=df['fivegram_pos'].str.split(r'\\s+').str\n",
    "            #df=df.query('word_pos == @word_list')\n",
    "            df=relemjoin(df,'word_pos')\n",
    "            df=pd.melt(df,id_vars=['word','decade','count'],value_vars=['r1_pos','r2_pos','r3_pos'])\n",
    "            #print(time.time()-t1)\n",
    "            return df\n",
    "        else:\n",
    "            phrases=df.loc[df.fivegram_pos.str.match(r'^[-a-z]+_noun\\s+[-a-z]+_noun\\s+[-a-z]+_.+\\s+[-a-z]+_.+\\s+[-a-z]+_.+$')]\n",
    "            cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^[-a-z]+_noun\\s+[-a-z]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_.+\\s+[a-z-]+_.+$')]\n",
    "\n",
    "            try:\n",
    "                phrases['modifier_pos'],phrases['head_pos'],phrases['r1_pos'],phrases['r2_pos'],phrases['r3_pos']=phrases['fivegram_pos'].str.split(r'\\s+').str\n",
    "                cdsm['modifier_pos'],cdsm['head_pos'],cdsm['r1_pos'],cdsm['r2_pos'],cdsm['r3_pos']=cdsm['fivegram_pos'].str.split(r'\\s+').str\n",
    "            except ValueError:\n",
    "                phrases=pd.DataFrame()\n",
    "                compounds=pd.DataFrame()\n",
    "                modifiers=pd.DataFrame()\n",
    "                heads=pd.DataFrame()\n",
    "                return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            phrases=relemjoin(phrases,'modifier_pos',lemmatize=False)\n",
    "            phrases=relemjoin(phrases,'head_pos')\n",
    "            cdsm=relemjoin(cdsm,'modifier_pos',lemmatize=False)\n",
    "            cdsm=relemjoin(cdsm,'head_pos')\n",
    "            \n",
    "            phrases=pd.melt(phrases,id_vars=['modifier','head','decade','count'],value_vars=['r1_pos','r2_pos','r3_pos'])\n",
    "            compounds=pd.melt(cdsm,id_vars=['modifier','head','decade','count'],value_vars=['r1_pos','r2_pos','r3_pos'])\n",
    "            modifiers=pd.melt(cdsm,id_vars=['modifier','decade','count'],value_vars=['head','r1_pos','r2_pos'])\n",
    "            heads=pd.melt(cdsm,id_vars=['head','decade','count'],value_vars=['modifier','r1_pos','r2_pos','r3_pos'])\n",
    "\n",
    "            return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            \n",
    "    elif align==\"mid1\":\n",
    "        if level==\"word\":\n",
    "            df=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+$')]\n",
    "            if len(df) == 0:\n",
    "                return df\n",
    "            \n",
    "            df['l1_pos'],df['word_pos'],df['r1_pos'],df['r2_pos'],df['r3_pos']=df['fivegram_pos'].str.split(r'\\s+').str\n",
    "            df=relemjoin(df,'word_pos')\n",
    "            df=pd.melt(df,id_vars=['word','decade','count'],value_vars=['l1_pos','r1_pos','r2_pos','r3_pos'])\n",
    "            return df\n",
    "        else:\n",
    "            phrases=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_.+\\s+[a-z-]+_.+$')]\n",
    "            cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_.+$')]\n",
    "\n",
    "            try:\n",
    "                phrases['l1_pos'],phrases['modifier_pos'],phrases['head_pos'],phrases['r1_pos'],phrases['r2_pos']=phrases['fivegram_pos'].str.split(r'\\s+').str\n",
    "                cdsm['l1_pos'],cdsm['modifier_pos'],cdsm['head_pos'],cdsm['r1_pos'],cdsm['r2_pos']=cdsm['fivegram_pos'].str.split(r'\\s+').str\n",
    "            except ValueError:\n",
    "                phrases=pd.DataFrame()\n",
    "                compounds=pd.DataFrame()\n",
    "                modifiers=pd.DataFrame()\n",
    "                heads=pd.DataFrame()\n",
    "                return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            phrases=relemjoin(phrases,'modifier_pos',lemmatize=False)\n",
    "            phrases=relemjoin(phrases,'head_pos')\n",
    "            cdsm=relemjoin(cdsm,'modifier_pos',lemmatize=False)\n",
    "            cdsm=relemjoin(cdsm,'head_pos')\n",
    "            \n",
    "            phrases=pd.melt(phrases,id_vars=['modifier','head','decade','count'],value_vars=['l1_pos','r1_pos','r2_pos'])\n",
    "            compounds=pd.melt(cdsm,id_vars=['modifier','head','decade','count'],value_vars=['l1_pos','r1_pos','r2_pos'])\n",
    "            modifiers=pd.melt(cdsm,id_vars=['modifier','decade','count'],value_vars=['head','l1_pos','r1_pos','r2_pos'])\n",
    "            heads=pd.melt(cdsm,id_vars=['head','decade','count'],value_vars=['modifier','l1_pos','r1_pos','r2_pos'])\n",
    "            return phrases,compounds,modifiers,heads\n",
    "    \n",
    "            \n",
    "    elif align==\"mid2\":\n",
    "        if level==\"word\":\n",
    "            df=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_.+\\s+[a-z-]+_.+$')]\n",
    "            if len(df) == 0:\n",
    "                return df\n",
    "            \n",
    "            df['l1_pos'],df['l2_pos'],df['word_pos'],df['r1_pos'],df['r2_pos']=df['fivegram_pos'].str.split(r'\\s+').str\n",
    "            df=relemjoin(df,'word_pos')\n",
    "            df=pd.melt(df,id_vars=['word','decade','count'],value_vars=['l1_pos','l2_pos','r1_pos','r2_pos'])\n",
    "            return df\n",
    "        else:\n",
    "            \n",
    "            phrases=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_.+$')]\n",
    "            cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_noun$')]\n",
    "\n",
    "            try:\n",
    "                phrases['l1_pos'],phrases['l2_pos'],phrases['modifier_pos'],phrases['head_pos'],phrases['r1_pos']=phrases['fivegram_pos'].str.split(r'\\s+').str\n",
    "                cdsm['l1_pos'],cdsm['l2_pos'],cdsm['modifier_pos'],cdsm['head_pos'],cdsm['r1_pos']=cdsm['fivegram_pos'].str.split(r'\\s+').str\n",
    "            except ValueError:\n",
    "                phrases=pd.DataFrame()\n",
    "                compounds=pd.DataFrame()\n",
    "                modifiers=pd.DataFrame()\n",
    "                heads=pd.DataFrame()\n",
    "                return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            phrases=relemjoin(phrases,'modifier_pos',lemmatize=False)\n",
    "            phrases=relemjoin(phrases,'head_pos')\n",
    "            cdsm=relemjoin(cdsm,'modifier_pos',lemmatize=False)\n",
    "            cdsm=relemjoin(cdsm,'head_pos')\n",
    "            \n",
    "            phrases=pd.melt(phrases,id_vars=['modifier','head','decade','count'],value_vars=['l1_pos','l2_pos','r1_pos'])\n",
    "            compounds=pd.melt(cdsm,id_vars=['modifier','head','decade','count'],value_vars=['l1_pos','l2_pos','r1_pos'])\n",
    "            modifiers=pd.melt(cdsm,id_vars=['modifier','decade','count'],value_vars=['head','l1_pos','l2_pos','r1_pos'])\n",
    "            heads=pd.melt(cdsm,id_vars=['head','decade','count'],value_vars=['modifier','l1_pos','l2_pos','r1_pos'])\n",
    "            return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            \n",
    "    elif align==\"mid3\":\n",
    "        df=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_.+$')]\n",
    "        if len(df)==0:\n",
    "            return df\n",
    "\n",
    "        df['l1_pos'],df['l2_pos'],df['word_pos'],df['r1_pos'],df['r2_pos']=df['fivegram_pos'].str.split(r'\\s+').str\n",
    "        df=relemjoin(df,'word_pos')\n",
    "        df=pd.melt(df,id_vars=['word','decade','count'],value_vars=['l1_pos','l2_pos','r1_pos','r2_pos'])\n",
    "        return df\n",
    "        \n",
    "    elif align==\"left\":\n",
    "        \n",
    "        if level==\"word\":\n",
    "            df=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun$')]\n",
    "            if len(df) == 0:\n",
    "                return df\n",
    "            _,df['l1_pos'],df['l2_pos'],df['l3_pos'],df['word_pos']=df['fivegram_pos'].str.split(r'\\s+').str\n",
    "            df=relemjoin(df,'word_pos')\n",
    "            df=pd.melt(df,id_vars=['word','decade','count'],value_vars=['l1_pos','l2_pos','l3_pos'])\n",
    "            return df\n",
    "        else:\n",
    "            phrases=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_noun$')]\n",
    "            cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_noun$')]\n",
    "            \n",
    "            try:\n",
    "                phrases['l1_pos'],phrases['l2_pos'],phrases['l3_pos'],phrases['modifier_pos'],phrases['head_pos']=phrases['fivegram_pos'].str.split(r'\\s+').str\n",
    "                cdsm['l1_pos'],cdsm['l2_pos'],cdsm['l3_pos'],cdsm['modifier_pos'],cdsm['head_pos']=cdsm['fivegram_pos'].str.split(r'\\s+').str\n",
    "            except ValueError:\n",
    "                phrases=pd.DataFrame()\n",
    "                compounds=pd.DataFrame()\n",
    "                modifiers=pd.DataFrame()\n",
    "                heads=pd.DataFrame()\n",
    "                return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            phrases=relemjoin(phrases,'modifier_pos',lemmatize=False)\n",
    "            phrases=relemjoin(phrases,'head_pos')\n",
    "            cdsm=relemjoin(cdsm,'modifier_pos',lemmatize=False)\n",
    "            cdsm=relemjoin(cdsm,'head_pos')\n",
    "            \n",
    "            phrases=pd.melt(phrases,id_vars=['modifier','head','decade','count'],value_vars=['l1_pos','l2_pos','l3_pos'])\n",
    "            compounds=pd.melt(cdsm,id_vars=['modifier','head','decade','count'],value_vars=['l1_pos','l2_pos','l3_pos'])\n",
    "            modifiers=pd.melt(cdsm,id_vars=['modifier','decade','count'],value_vars=['head','l1_pos','l2_pos','l3_pos'])\n",
    "            heads=pd.melt(cdsm,id_vars=['head','decade','count'],value_vars=['modifier','l1_pos','l2_pos','l3_pos'])\n",
    "            return phrases,compounds,modifiers,heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_reducer(df):\n",
    "    if len(df)==0:\n",
    "        return df\n",
    "    df[\"variable\"]=df[\"variable\"].str.replace(r\"_pos\",\"\")\n",
    "    df[\"context\"],df[\"context_pos\"]=df['value'].str.split('_', 1).str\n",
    "    df.replace(spelling_replacement,inplace=True)\n",
    "    df=df.loc[df.context_pos.isin([\"noun\",\"adj\",\"adv\",\"verb\"])]\n",
    "    df.replace(adv_replacement,inplace=True)\n",
    "    df['context_pos']=df['context_pos'].str[0]\n",
    "    if len(df)==0:\n",
    "        return df\n",
    "    df['context']=np.vectorize(lemma_maker)(df['context'], df['context_pos'])\n",
    "    df['context']=df['context']+\"_\"+df['context_pos']\n",
    "    df.query('context in @contextwords',inplace=True)\n",
    "    #df.reset_index(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdsm_word_reducer(df):\n",
    "    rightgram=syntactic_reducer(df,align=\"right\",level=\"word\")\n",
    "    rightgram=context_reducer(rightgram)\n",
    "    \n",
    "    mid1gram=syntactic_reducer(df,align=\"mid1\",level=\"word\")\n",
    "    mid1gram=context_reducer(mid1gram)\n",
    "    \n",
    "    mid2gram=syntactic_reducer(df,align=\"mid2\",level=\"word\")\n",
    "    mid2gram=context_reducer(mid2gram)\n",
    "    \n",
    "    mid3gram=syntactic_reducer(df,align=\"mid3\",level=\"word\")\n",
    "    mid3gram=context_reducer(mid3gram)\n",
    "    \n",
    "    leftgram=syntactic_reducer(df,align=\"left\",level=\"word\")\n",
    "    leftgram=context_reducer(leftgram)    \n",
    "    \n",
    "    words_df=pd.concat([rightgram,mid1gram,mid2gram,mid3gram,leftgram],ignore_index=True,sort=False)\n",
    "    words_df.dropna(inplace=True)\n",
    "    words_df=words_df.query('word in @word_list')\n",
    "    words_df=words_df.groupby(['word','context','decade'])['count'].sum().to_frame()\n",
    "    words_df.reset_index(inplace=True)\n",
    "    words_df.decade=words_df.decade.astype(\"int32\")\n",
    "    print(words_df.shape)\n",
    "    return words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdsm_reducer(df):\n",
    "    phrase_rightgram,compound_rightgram,modifier_rightgram,head_rightgram=syntactic_reducer(df,align=\"right\")\n",
    "    phrase_rightgram=context_reducer(phrase_rightgram)\n",
    "    compound_rightgram=context_reducer(compound_rightgram)\n",
    "    modifier_rightgram=context_reducer(modifier_rightgram)\n",
    "    head_rightgram=context_reducer(head_rightgram)\n",
    "\n",
    "\n",
    "    phrase_mid1gram,compound_mid1gram,modifier_mid1gram,head_mid1gram=syntactic_reducer(df,align=\"mid1\")\n",
    "    phrase_mid1gram=context_reducer(phrase_mid1gram)\n",
    "    compound_mid1gram=context_reducer(compound_mid1gram)\n",
    "    modifier_mid1gram=context_reducer(modifier_mid1gram)\n",
    "    head_mid1gram=context_reducer(head_mid1gram)\n",
    " \n",
    "\n",
    "    phrase_mid2gram,compound_mid2gram,modifier_mid2gram,head_mid2gram=syntactic_reducer(df,align=\"mid2\")\n",
    "    phrase_mid2gram=context_reducer(phrase_mid2gram)\n",
    "    compound_mid2gram=context_reducer(compound_mid2gram)\n",
    "    modifier_mid2gram=context_reducer(modifier_mid2gram)\n",
    "    head_mid2gram=context_reducer(head_mid2gram)\n",
    "    \n",
    "    phrase_leftgram,compound_leftgram,modifier_leftgram,head_leftgram=syntactic_reducer(df,align=\"left\")\n",
    "    phrase_leftgram=context_reducer(phrase_leftgram)\n",
    "    compound_leftgram=context_reducer(compound_leftgram)\n",
    "    modifier_leftgram=context_reducer(modifier_leftgram)\n",
    "    head_leftgram=context_reducer(head_leftgram)\n",
    "    \n",
    "    compounds=pd.concat([compound_rightgram,compound_mid1gram,compound_mid2gram,compound_leftgram],ignore_index=True)\n",
    "    modifiers=pd.concat([modifier_rightgram,modifier_mid1gram,modifier_mid2gram,modifier_leftgram],ignore_index=True)\n",
    "    heads=pd.concat([head_rightgram,head_mid1gram,head_mid2gram,head_leftgram],ignore_index=True)\n",
    "\n",
    "    phrases=pd.concat([phrase_rightgram,phrase_mid1gram,phrase_mid2gram,phrase_leftgram],ignore_index=True)\n",
    "    phrases.dropna(inplace=True)\n",
    "    phrases=phrases.groupby(['modifier','head','context','decade'])['count'].sum().to_frame()\n",
    "    phrases.reset_index(inplace=True)\n",
    "    phrases.decade=phrases.decade.astype(\"int32\")\n",
    "    \n",
    "    compounds.dropna(inplace=True)\n",
    "    compounds=compounds.groupby(['modifier','head','context','decade'])['count'].sum().to_frame()\n",
    "    compounds.reset_index(inplace=True)\n",
    "    compounds.decade=compounds.decade.astype(\"int32\")\n",
    "    \n",
    "    modifiers.dropna(inplace=True)\n",
    "    modifiers=modifiers.groupby(['modifier','context','decade'])['count'].sum().to_frame()\n",
    "    modifiers.reset_index(inplace=True)\n",
    "    modifiers.decade=modifiers.decade.astype(\"int32\")\n",
    "    \n",
    "    heads.dropna(inplace=True)\n",
    "    heads=heads.groupby(['head','context','decade'])['count'].sum().to_frame()\n",
    "    heads.reset_index(inplace=True)\n",
    "    heads.decade=heads.decade.astype(\"int32\")\n",
    "    return compounds,modifiers,heads,phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df, context_type=\"independant_word\",num_cores = 70):\n",
    "    num_partitions = num_cores\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    print(\"Done splitting the datasets\")\n",
    "    pool = Pool(num_cores)\n",
    "\n",
    "    cur_time=time.time()\n",
    "    print(\"Starting parallelizing\")\n",
    "    if context_type==\"dependant\":\n",
    "\n",
    "        results=pool.map_async(cdsm_reducer,df_split)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        results=results.get()\n",
    "\n",
    "        \n",
    "        print(\"Done parallelizing\")\n",
    "        print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "        compound_list = [ result[0] for result in results]\n",
    "        compounds=pd.concat(compound_list,ignore_index=True)\n",
    "        compounds=compounds.groupby(['modifier','head','context','decade'])['count'].sum().to_frame()\n",
    "        compounds.reset_index(inplace=True)\n",
    "        \n",
    "        if not isfile(\"/data/dharp/compounding/datasets/compounds.csv\"):\n",
    "            compounds.to_csv(\"/data/dharp/compounding/datasets/compounds.csv\",sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            compounds.to_csv(\"/data/dharp/compounding/datasets/compounds.csv\", mode='a',sep=\"\\t\", header=False,index=False)\n",
    "        \n",
    "        \n",
    "        modifier_list = [ result[1] for result in results]\n",
    "        modifiers=pd.concat(modifier_list,ignore_index=True)\n",
    "        modifiers=modifiers.groupby(['modifier','context','decade'])['count'].sum().to_frame()\n",
    "        modifiers.reset_index(inplace=True)\n",
    "\n",
    "        if not isfile(\"/data/dharp/compounding/datasets/modifiers.csv\"):\n",
    "            modifiers.to_csv(\"/data/dharp/compounding/datasets/modifiers.csv\",sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            modifiers.to_csv(\"/data/dharp/compounding/datasets/modifiers.csv\", mode='a', header=False,index=False)\n",
    "        \n",
    "        head_list = [ result[2] for result in results]\n",
    "        heads=pd.concat(head_list,ignore_index=True)\n",
    "        heads=heads.groupby(['head','context','decade'])['count'].sum().to_frame()\n",
    "        heads.reset_index(inplace=True)\n",
    "\n",
    "        if not isfile(\"/data/dharp/compounding/datasets/heads.csv\"):\n",
    "            heads.to_csv(\"/data/dharp/compounding/datasets/heads.csv\",sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            heads.to_csv(\"/data/dharp/compounding/datasets/heads.csv\", mode='a', header=False,index=False)\n",
    "            \n",
    "        phrase_list = [ result[3] for result in results]\n",
    "        phrases=pd.concat(phrase_list,ignore_index=True)\n",
    "        phrases=phrases.groupby(['modifier','head','context','decade'])['count'].sum().to_frame()\n",
    "        phrases.reset_index(inplace=True)\n",
    "        \n",
    "        if not isfile(\"/data/dharp/compounding/datasets/phrases.csv\"):\n",
    "            phrases.to_csv(\"/data/dharp/compounding/datasets/phrases.csv\",sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            phrases.to_csv(\"/data/dharp/compounding/datasets/phrases.csv\", mode='a', header=False,index=False)\n",
    "\n",
    "    elif context_type==\"independant_word\":\n",
    "        words_list=[]\n",
    "        results=pool.map_async(cdsm_word_reducer,df_split)\n",
    "  \n",
    "        \n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        print(\"Done parallelizing\")\n",
    "        print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "        words_list=results.get()\n",
    "        words = pd.concat(words_list,ignore_index=True,sort=False)\n",
    "        words=words.groupby(['word','context','decade'])['count'].sum().to_frame()\n",
    "        words.reset_index(inplace=True)\n",
    "        print(words.shape)\n",
    "                \n",
    "        if not isfile(\"/data/dharp/compounding/datasets/words_test.csv\"):\n",
    "            words.to_csv(\"/data/dharp/compounding/datasets/words_test.csv\",sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            words.to_csv(\"/data/dharp/compounding/datasets/words_test.csv\", mode='a',sep=\"\\t\", header=False,index=False)\n",
    "        \n",
    "    print(\"Done concatenations \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_assign(df,split_col,splitter,col_names):\n",
    "    temp_cols=df[split_col].str.split(splitter,len(col_names)).str\n",
    "    df = df.assign(**{k:temp_cols[i] for i,k in enumerate(col_names)})\n",
    "    #df.drop(split_col,axis=1,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "chunk_size=200_000_000\n",
    "file_id=0\n",
    "for start in range(0, entire_df.shape[0], chunk_size):\n",
    "    entire_df.iloc[start:start + chunk_size].to_hdf('/data/dharp/compounding/datasets/entire_df_'+str(file_id)+'.h5', key='df', format='fixed', complevel=9, complib='zlib')\n",
    "    file_id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/dharp/compounding/datasets/entire_df_10_.h5 is read in\n"
     ]
    }
   ],
   "source": [
    "num_cores=20\n",
    "tmp_df=pd.read_hdf(files[0], key='df')\n",
    "print(files[0],'is read in')\n",
    "tmp_df_klein=tmp_df.head(10_000_000)\n",
    "df_split = np.array_split(tmp_df_klein, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split num: 1\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "(43772, 4)\n",
      "(43985, 4)\n",
      "(46741, 4)\n",
      "(43474, 4)\n",
      "(53792, 4)\n",
      "(0, 4)\n",
      "(65069, 4)\n",
      "(54660, 4)\n",
      "(71530, 4)\n",
      "(78005, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(89891, 4)\n",
      "(0, 4)\n",
      "(99607, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "Done parallelizing\n",
      "Total time taken 138 secs\n",
      "(597170, 4)\n",
      "Done concatenations \n",
      "\n",
      "Split num: 2\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "(52573, 4)\n",
      "(0, 4)\n",
      "(53573, 4)\n",
      "(49930, 4)\n",
      "(64090, 4)\n",
      "(83427, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(91246, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "Done parallelizing\n",
      "Total time taken 157 secs\n",
      "(362679, 4)\n",
      "Done concatenations \n",
      "\n",
      "Split num: 3\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "(43888, 4)\n",
      "(44514, 4)\n",
      "(0, 4)\n",
      "(46085, 4)\n",
      "(64409, 4)\n",
      "(0, 4)\n",
      "(69476, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(87465, 4)\n",
      "(80191, 4)\n",
      "(0, 4)\n",
      "(87954, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "Done parallelizing\n",
      "Total time taken 145 secs\n",
      "(467570, 4)\n",
      "Done concatenations \n",
      "\n",
      "Split num: 4\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "(50871, 4)\n",
      "(50591, 4)\n",
      "(49244, 4)\n",
      "(53237, 4)\n",
      "(50396, 4)\n",
      "(53465, 4)\n",
      "(70749, 4)\n",
      "(0, 4)\n",
      "(67870, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(91105, 4)\n",
      "(81938, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "Done parallelizing\n",
      "Total time taken 142 secs\n",
      "(540834, 4)\n",
      "Done concatenations \n",
      "\n",
      "Split num: 5\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "(48117, 4)\n",
      "(43106, 4)\n",
      "(49386, 4)\n",
      "(61001, 4)\n",
      "(61544, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(104966, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(96328, 4)\n",
      "Done parallelizing\n",
      "Total time taken 152 secs\n",
      "(423105, 4)\n",
      "Done concatenations \n",
      "\n",
      "Split num: 6\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "(43005, 4)\n",
      "(54129, 4)\n",
      "(0, 4)\n",
      "(51208, 4)\n",
      "(57364, 4)\n",
      "(51154, 4)\n",
      "(58528, 4)\n",
      "(74915, 4)\n",
      "(71697, 4)\n",
      "(0, 4)\n",
      "(81521, 4)\n",
      "(0, 4)\n",
      "(82901, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(89017, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "Done parallelizing\n",
      "Total time taken 137 secs\n",
      "(618226, 4)\n",
      "Done concatenations \n",
      "\n",
      "Split num: 7\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "(47472, 4)\n",
      "(45978, 4)\n",
      "(54411, 4)\n",
      "(51871, 4)\n",
      "(62600, 4)\n",
      "(53474, 4)\n",
      "(93196, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(112611, 4)\n",
      "Done parallelizing\n",
      "Total time taken 159 secs\n",
      "(471118, 4)\n",
      "Done concatenations \n",
      "\n",
      "Split num: 8\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "(43995, 4)\n",
      "(0, 4)\n",
      "(44083, 4)\n",
      "(50694, 4)\n",
      "(60244, 4)\n",
      "(58349, 4)\n",
      "(0, 4)\n",
      "(74105, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(91631, 4)\n",
      "(0, 4)\n",
      "(86861, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "Done parallelizing\n",
      "Total time taken 141 secs\n",
      "(453094, 4)\n",
      "Done concatenations \n",
      "\n",
      "Split num: 9\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "(53886, 4)\n",
      "(58327, 4)\n",
      "(41808, 4)\n",
      "(61730, 4)\n",
      "(53621, 4)\n",
      "(0, 4)\n",
      "(54502, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(90841, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(101724, 4)\n",
      "(0, 4)\n",
      "Done parallelizing\n",
      "Total time taken 144 secs\n",
      "(465098, 4)\n",
      "Done concatenations \n",
      "\n",
      "Split num: 10\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "(40901, 4)\n",
      "(53420, 4)\n",
      "(0, 4)\n",
      "(58514, 4)\n",
      "(69609, 4)\n",
      "(0, 4)\n",
      "(95979, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n",
      "Done parallelizing\n",
      "Total time taken 152 secs\n",
      "(294747, 4)\n",
      "Done concatenations \n",
      "\n",
      "Done writing to files\n"
     ]
    }
   ],
   "source": [
    "for num,df in enumerate(df_split):\n",
    "    print(\"Split num:\",num+1)\n",
    "    parallelize_dataframe(df,context_type=\"independant_word\",num_cores=num_cores)\n",
    "print(\"Done writing to files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/dharp/compounding/datasets/entire_df_10_.h5 is read in\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-061a1ca82d5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtmp_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'df'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'is read in'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Split num:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/dharp/packages/miniconda3/lib/python3.6/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36marray_split\u001b[0;34m(ary, indices_or_sections, axis)\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiv_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiv_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m         \u001b[0msub_arys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msub_arys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/dharp/packages/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mswapaxes\u001b[0;34m(a, axis1, axis2)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \"\"\"\n\u001b[0;32m--> 549\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'swapaxes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/dharp/packages/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/dharp/packages/miniconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mswapaxes\u001b[0;34m(self, axis1, axis2, copy)\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/dharp/packages/miniconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m   5112\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5113\u001b[0m         \"\"\"\n\u001b[0;32m-> 5114\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5115\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/dharp/packages/miniconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep, mgr)\u001b[0m\n\u001b[1;32m   3918\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3919\u001b[0m         return self.apply('copy', axes=new_axes, deep=deep,\n\u001b[0;32m-> 3920\u001b[0;31m                           do_integrity_check=False)\n\u001b[0m\u001b[1;32m   3921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3922\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mas_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/dharp/packages/miniconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[1;32m   3579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3580\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mgr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3581\u001b[0;31m             \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3582\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/dharp/packages/miniconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep, mgr)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_block_same_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_cores=20\n",
    "tmp_df=pd.read_hdf(files[0], key='df')\n",
    "print(files[0],'is read in')\n",
    "df_split = np.array_split(tmp_df, 10)\n",
    "for num,df in enumerate(df_split):\n",
    "    print(\"Split num:\",num+1)\n",
    "    parallelize_dataframe(df,context_type=\"independant_word\",num_cores=num_cores)\n",
    "print(\"Done writing to files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = '/data/dharp/compounding/datasets/entire_df_0_.h5'\n",
    "tmp_df=pd.read_hdf(files[0], key='df')\n",
    "#tmp_df=tmp_df.head(10_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 200000000 entries, 2000000000 to 2199999999\n",
      "Data columns (total 3 columns):\n",
      "fivegram_pos    object\n",
      "decade          int32\n",
      "count           int64\n",
      "dtypes: int32(1), int64(1), object(1)\n",
      "memory usage: 5.2+ GB\n"
     ]
    }
   ],
   "source": [
    "tmp_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
