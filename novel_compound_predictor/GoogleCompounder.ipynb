{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join,getsize\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "import pickle as pkl\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import warnings\n",
    "#warnings.simplefilter(action='ignore', category=ResourceWarning)\n",
    "np.random.seed(seed=1991)\n",
    "import tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_pattern=r'.+_.+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.+_.+'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_files=glob.glob(\"/data/dharp/compounding/datasets/*.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_df=pd.read_pickle('/data/dharp/compounding/pkl_datasets/wo.pkl')\n",
    "trial_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pattern=trial_df.loc[trial_df.fivegram_pos.str.contains(\".*_noun\\s'_noun\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21215                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21216                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21217                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21218                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21219                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21220                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21221                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21222                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21223                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21224                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21225                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21226                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21227                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21228                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21229                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21230                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21231                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21232                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21233                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21234                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21235                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21236                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21237                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21238                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21239                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21240                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21241                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "21242                  [woe_noun, is_verb, me_pron, t_noun]\n",
       "247546            [wolf_noun, and_conj, the_det, lamb_noun]\n",
       "247547            [wolf_noun, and_conj, the_det, lamb_noun]\n",
       "                                  ...                      \n",
       "106245814    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245815    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245816    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245817    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245818    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245819    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245820    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245821    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245822    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245823    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245824    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245825    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245826    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245827    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245828    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245829    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245830    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245831    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245832    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245833    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245834    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245835    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245836    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245837    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245838    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245839    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245840    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245841    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245842    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "106245843    [wounded_verb, at_adp, gaines_noun, mill_noun]\n",
       "Name: fivegram_pos, Length: 44865, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_split=new_pattern.fivegram_pos.str.replace(r\"'_noun\",'')\n",
    "to_split.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "br_to_us=pd.read_excel(\"Book.xlsx\",header=1)\n",
    "br_to_us_dict=dict(zip(br_to_us.UK.tolist(),br_to_us.US.tolist()))\n",
    "\n",
    "contextwords_df=pkl.load( open( \"contexts.pkl\", \"rb\" ) )\n",
    "\n",
    "\n",
    "\n",
    "spelling_replacement={'context':br_to_us_dict,'modifier':br_to_us_dict,'head':br_to_us_dict,'word':br_to_us_dict}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_maker(x, y):\n",
    "    #print(lemmatizer(x,y)[0])\n",
    "    return lemmatizer(x,y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relemjoin(df,col_name):\n",
    "    new_col=col_name.split('_')[0]\n",
    "    new_col_pos=new_col[0]+\"_pos\"\n",
    "    df[new_col]=df[col_name].str.split('_', 1).str[0]\n",
    "    df[new_col_pos]=\"n\"\n",
    "    df[new_col]=np.vectorize(lemma_maker)(df[new_col], df[new_col_pos])\n",
    "    df[new_col]=df[new_col]+\"_n\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntactic_reducer(df,align,level=None):\n",
    "    if len(df) == 0:\n",
    "        print(\"Am here\")\n",
    "        return df\n",
    "    if align==\"right\":\n",
    "        if level==\"word\":\n",
    "            #t1=time.time()\n",
    "            df=df.loc[df.fivegram_pos.str.match(r'^[-a-z]+_noun\\s+.+_.+\\s+.+_.+\\s+[-a-z+_.+\\s+[-a-z]+_.+$')]\n",
    "            if len(df) == 0:\n",
    "                return df\n",
    "            \n",
    "            df['word_pos'],df['r1_pos'],df['r2_pos'],df['r3_pos'],_=df['fivegram_pos'].str.split(r'\\s+').str\n",
    "            #df=df.query('word_pos == @word_list')\n",
    "            df=relemjoin(df,'word_pos')\n",
    "            df=pd.melt(df,id_vars=['word','decade','count'],value_vars=['r1_pos','r2_pos','r3_pos'])\n",
    "            #print(time.time()-t1)\n",
    "            return df\n",
    "        else:\n",
    "            phrases=df.loc[df.fivegram_pos.str.match(r'^[-a-z]+_noun\\s+[-a-z]+_noun\\s+[-a-z]+_.+\\s+[-a-z]+_.+\\s+[-a-z]+_.+$')]\n",
    "            cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^[-a-z]+_noun\\s+[-a-z]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_.+\\s+[a-z-]+_.+$')]\n",
    "\n",
    "            try:\n",
    "                phrases['modifier_pos'],phrases['head_pos'],phrases['r1_pos'],phrases['r2_pos'],phrases['r3_pos']=phrases['fivegram_pos'].str.split(r'\\s+').str\n",
    "                cdsm['modifier_pos'],cdsm['head_pos'],cdsm['r1_pos'],cdsm['r2_pos'],cdsm['r3_pos']=cdsm['fivegram_pos'].str.split(r'\\s+').str\n",
    "            except ValueError:\n",
    "                phrases=pd.DataFrame()\n",
    "                compounds=pd.DataFrame()\n",
    "                modifiers=pd.DataFrame()\n",
    "                heads=pd.DataFrame()\n",
    "                return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            phrases=relemjoin(phrases,'modifier_pos',lemmatize=False)\n",
    "            phrases=relemjoin(phrases,'head_pos')\n",
    "            cdsm=relemjoin(cdsm,'modifier_pos',lemmatize=False)\n",
    "            cdsm=relemjoin(cdsm,'head_pos')\n",
    "            \n",
    "            phrases=pd.melt(phrases,id_vars=['modifier','head','decade','count'],value_vars=['r1_pos','r2_pos','r3_pos'])\n",
    "            compounds=pd.melt(cdsm,id_vars=['modifier','head','decade','count'],value_vars=['r1_pos','r2_pos','r3_pos'])\n",
    "            modifiers=pd.melt(cdsm,id_vars=['modifier','decade','count'],value_vars=['head','r1_pos','r2_pos'])\n",
    "            heads=pd.melt(cdsm,id_vars=['head','decade','count'],value_vars=['modifier','r1_pos','r2_pos','r3_pos'])\n",
    "\n",
    "            return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            \n",
    "    elif align==\"mid1\":\n",
    "        if level==\"word\":\n",
    "            df=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+$')]\n",
    "            if len(df) == 0:\n",
    "                return df\n",
    "            \n",
    "            df['l1_pos'],df['word_pos'],df['r1_pos'],df['r2_pos'],df['r3_pos']=df['fivegram_pos'].str.split(r'\\s+').str\n",
    "            df=relemjoin(df,'word_pos')\n",
    "            df=pd.melt(df,id_vars=['word','decade','count'],value_vars=['l1_pos','r1_pos','r2_pos','r3_pos'])\n",
    "            return df\n",
    "        else:\n",
    "            phrases=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_.+\\s+[a-z-]+_.+$')]\n",
    "            cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_.+$')]\n",
    "\n",
    "            try:\n",
    "                phrases['l1_pos'],phrases['modifier_pos'],phrases['head_pos'],phrases['r1_pos'],phrases['r2_pos']=phrases['fivegram_pos'].str.split(r'\\s+').str\n",
    "                cdsm['l1_pos'],cdsm['modifier_pos'],cdsm['head_pos'],cdsm['r1_pos'],cdsm['r2_pos']=cdsm['fivegram_pos'].str.split(r'\\s+').str\n",
    "            except ValueError:\n",
    "                phrases=pd.DataFrame()\n",
    "                compounds=pd.DataFrame()\n",
    "                modifiers=pd.DataFrame()\n",
    "                heads=pd.DataFrame()\n",
    "                return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            phrases=relemjoin(phrases,'modifier_pos',lemmatize=False)\n",
    "            phrases=relemjoin(phrases,'head_pos')\n",
    "            cdsm=relemjoin(cdsm,'modifier_pos',lemmatize=False)\n",
    "            cdsm=relemjoin(cdsm,'head_pos')\n",
    "            \n",
    "            phrases=pd.melt(phrases,id_vars=['modifier','head','decade','count'],value_vars=['l1_pos','r1_pos','r2_pos'])\n",
    "            compounds=pd.melt(cdsm,id_vars=['modifier','head','decade','count'],value_vars=['l1_pos','r1_pos','r2_pos'])\n",
    "            modifiers=pd.melt(cdsm,id_vars=['modifier','decade','count'],value_vars=['head','l1_pos','r1_pos','r2_pos'])\n",
    "            heads=pd.melt(cdsm,id_vars=['head','decade','count'],value_vars=['modifier','l1_pos','r1_pos','r2_pos'])\n",
    "            return phrases,compounds,modifiers,heads\n",
    "    \n",
    "            \n",
    "    elif align==\"mid2\":\n",
    "        if level==\"word\":\n",
    "            df=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_.+\\s+[a-z-]+_.+$')]\n",
    "            if len(df) == 0:\n",
    "                return df\n",
    "            \n",
    "            df['l1_pos'],df['l2_pos'],df['word_pos'],df['r1_pos'],df['r2_pos']=df['fivegram_pos'].str.split(r'\\s+').str\n",
    "            df=relemjoin(df,'word_pos')\n",
    "            df=pd.melt(df,id_vars=['word','decade','count'],value_vars=['l1_pos','l2_pos','r1_pos','r2_pos'])\n",
    "            return df\n",
    "        else:\n",
    "            \n",
    "            phrases=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_.+$')]\n",
    "            cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_noun$')]\n",
    "\n",
    "            try:\n",
    "                phrases['l1_pos'],phrases['l2_pos'],phrases['modifier_pos'],phrases['head_pos'],phrases['r1_pos']=phrases['fivegram_pos'].str.split(r'\\s+').str\n",
    "                cdsm['l1_pos'],cdsm['l2_pos'],cdsm['modifier_pos'],cdsm['head_pos'],cdsm['r1_pos']=cdsm['fivegram_pos'].str.split(r'\\s+').str\n",
    "            except ValueError:\n",
    "                phrases=pd.DataFrame()\n",
    "                compounds=pd.DataFrame()\n",
    "                modifiers=pd.DataFrame()\n",
    "                heads=pd.DataFrame()\n",
    "                return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            phrases=relemjoin(phrases,'modifier_pos',lemmatize=False)\n",
    "            phrases=relemjoin(phrases,'head_pos')\n",
    "            cdsm=relemjoin(cdsm,'modifier_pos',lemmatize=False)\n",
    "            cdsm=relemjoin(cdsm,'head_pos')\n",
    "            \n",
    "            phrases=pd.melt(phrases,id_vars=['modifier','head','decade','count'],value_vars=['l1_pos','l2_pos','r1_pos'])\n",
    "            compounds=pd.melt(cdsm,id_vars=['modifier','head','decade','count'],value_vars=['l1_pos','l2_pos','r1_pos'])\n",
    "            modifiers=pd.melt(cdsm,id_vars=['modifier','decade','count'],value_vars=['head','l1_pos','l2_pos','r1_pos'])\n",
    "            heads=pd.melt(cdsm,id_vars=['head','decade','count'],value_vars=['modifier','l1_pos','l2_pos','r1_pos'])\n",
    "            return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            \n",
    "    elif align==\"mid3\":\n",
    "        df=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_.+$')]\n",
    "        if len(df)==0:\n",
    "            return df\n",
    "\n",
    "        df['l1_pos'],df['l2_pos'],df['word_pos'],df['r1_pos'],df['r2_pos']=df['fivegram_pos'].str.split(r'\\s+').str\n",
    "        df=relemjoin(df,'word_pos')\n",
    "        df=pd.melt(df,id_vars=['word','decade','count'],value_vars=['l1_pos','l2_pos','r1_pos','r2_pos'])\n",
    "        return df\n",
    "        \n",
    "    elif align==\"left\":\n",
    "        \n",
    "        if level==\"word\":\n",
    "            df=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun$')]\n",
    "            if len(df) == 0:\n",
    "                return df\n",
    "            _,df['l1_pos'],df['l2_pos'],df['l3_pos'],df['word_pos']=df['fivegram_pos'].str.split(r'\\s+').str\n",
    "            df=relemjoin(df,'word_pos')\n",
    "            df=pd.melt(df,id_vars=['word','decade','count'],value_vars=['l1_pos','l2_pos','l3_pos'])\n",
    "            return df\n",
    "        else:\n",
    "            phrases=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_noun$')]\n",
    "            cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_noun$')]\n",
    "            \n",
    "            try:\n",
    "                phrases['l1_pos'],phrases['l2_pos'],phrases['l3_pos'],phrases['modifier_pos'],phrases['head_pos']=phrases['fivegram_pos'].str.split(r'\\s+').str\n",
    "                cdsm['l1_pos'],cdsm['l2_pos'],cdsm['l3_pos'],cdsm['modifier_pos'],cdsm['head_pos']=cdsm['fivegram_pos'].str.split(r'\\s+').str\n",
    "            except ValueError:\n",
    "                phrases=pd.DataFrame()\n",
    "                compounds=pd.DataFrame()\n",
    "                modifiers=pd.DataFrame()\n",
    "                heads=pd.DataFrame()\n",
    "                return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            phrases=relemjoin(phrases,'modifier_pos',lemmatize=False)\n",
    "            phrases=relemjoin(phrases,'head_pos')\n",
    "            cdsm=relemjoin(cdsm,'modifier_pos',lemmatize=False)\n",
    "            cdsm=relemjoin(cdsm,'head_pos')\n",
    "            \n",
    "            phrases=pd.melt(phrases,id_vars=['modifier','head','decade','count'],value_vars=['l1_pos','l2_pos','l3_pos'])\n",
    "            compounds=pd.melt(cdsm,id_vars=['modifier','head','decade','count'],value_vars=['l1_pos','l2_pos','l3_pos'])\n",
    "            modifiers=pd.melt(cdsm,id_vars=['modifier','decade','count'],value_vars=['head','l1_pos','l2_pos','l3_pos'])\n",
    "            heads=pd.melt(cdsm,id_vars=['head','decade','count'],value_vars=['modifier','l1_pos','l2_pos','l3_pos'])\n",
    "            return phrases,compounds,modifiers,heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_reducer(df):\n",
    "    if len(df)==0:\n",
    "        return df\n",
    "    df[\"variable\"]=df[\"variable\"].str.replace(r\"_pos\",\"\")\n",
    "    df[\"context\"],df[\"context_pos\"]=df['value'].str.split('_', 1).str\n",
    "    df.replace(spelling_replacement,inplace=True)\n",
    "    df=df.loc[df.context_pos.isin([\"noun\",\"adj\",\"adv\",\"verb\"])]\n",
    "    df.replace(adv_replacement,inplace=True)\n",
    "    df['context_pos']=df['context_pos'].str[0]\n",
    "    if len(df)==0:\n",
    "        return df\n",
    "    df['context']=np.vectorize(lemma_maker)(df['context'], df['context_pos'])\n",
    "    df['context']=df['context']+\"_\"+df['context_pos']\n",
    "    df.query('context in @contextwords',inplace=True)\n",
    "    #df.reset_index(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdsm_word_reducer(df):\n",
    "    rightgram=syntactic_reducer(df,align=\"right\",level=\"word\")\n",
    "    rightgram=context_reducer(rightgram)\n",
    "    \n",
    "    mid1gram=syntactic_reducer(df,align=\"mid1\",level=\"word\")\n",
    "    mid1gram=context_reducer(mid1gram)\n",
    "    \n",
    "    mid2gram=syntactic_reducer(df,align=\"mid2\",level=\"word\")\n",
    "    mid2gram=context_reducer(mid2gram)\n",
    "    \n",
    "    mid3gram=syntactic_reducer(df,align=\"mid3\",level=\"word\")\n",
    "    mid3gram=context_reducer(mid3gram)\n",
    "    \n",
    "    leftgram=syntactic_reducer(df,align=\"left\",level=\"word\")\n",
    "    leftgram=context_reducer(leftgram)    \n",
    "    \n",
    "    words_df=pd.concat([rightgram,mid1gram,mid2gram,mid3gram,leftgram],ignore_index=True,sort=False)\n",
    "    words_df.dropna(inplace=True)\n",
    "    words_df=words_df.query('word in @word_list')\n",
    "    words_df=words_df.groupby(['word','context','decade'])['count'].sum().to_frame()\n",
    "    words_df.reset_index(inplace=True)\n",
    "    words_df.decade=words_df.decade.astype(\"int32\")\n",
    "    print(words_df.shape)\n",
    "    return words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdsm_reducer(df):\n",
    "    phrase_rightgram,compound_rightgram,modifier_rightgram,head_rightgram=syntactic_reducer(df,align=\"right\")\n",
    "    phrase_rightgram=context_reducer(phrase_rightgram)\n",
    "    compound_rightgram=context_reducer(compound_rightgram)\n",
    "    modifier_rightgram=context_reducer(modifier_rightgram)\n",
    "    head_rightgram=context_reducer(head_rightgram)\n",
    "\n",
    "\n",
    "    phrase_mid1gram,compound_mid1gram,modifier_mid1gram,head_mid1gram=syntactic_reducer(df,align=\"mid1\")\n",
    "    phrase_mid1gram=context_reducer(phrase_mid1gram)\n",
    "    compound_mid1gram=context_reducer(compound_mid1gram)\n",
    "    modifier_mid1gram=context_reducer(modifier_mid1gram)\n",
    "    head_mid1gram=context_reducer(head_mid1gram)\n",
    " \n",
    "\n",
    "    phrase_mid2gram,compound_mid2gram,modifier_mid2gram,head_mid2gram=syntactic_reducer(df,align=\"mid2\")\n",
    "    phrase_mid2gram=context_reducer(phrase_mid2gram)\n",
    "    compound_mid2gram=context_reducer(compound_mid2gram)\n",
    "    modifier_mid2gram=context_reducer(modifier_mid2gram)\n",
    "    head_mid2gram=context_reducer(head_mid2gram)\n",
    "    \n",
    "    phrase_leftgram,compound_leftgram,modifier_leftgram,head_leftgram=syntactic_reducer(df,align=\"left\")\n",
    "    phrase_leftgram=context_reducer(phrase_leftgram)\n",
    "    compound_leftgram=context_reducer(compound_leftgram)\n",
    "    modifier_leftgram=context_reducer(modifier_leftgram)\n",
    "    head_leftgram=context_reducer(head_leftgram)\n",
    "    \n",
    "    compounds=pd.concat([compound_rightgram,compound_mid1gram,compound_mid2gram,compound_leftgram],ignore_index=True)\n",
    "    modifiers=pd.concat([modifier_rightgram,modifier_mid1gram,modifier_mid2gram,modifier_leftgram],ignore_index=True)\n",
    "    heads=pd.concat([head_rightgram,head_mid1gram,head_mid2gram,head_leftgram],ignore_index=True)\n",
    "\n",
    "    phrases=pd.concat([phrase_rightgram,phrase_mid1gram,phrase_mid2gram,phrase_leftgram],ignore_index=True)\n",
    "    phrases.dropna(inplace=True)\n",
    "    phrases=phrases.groupby(['modifier','head','context','decade'])['count'].sum().to_frame()\n",
    "    phrases.reset_index(inplace=True)\n",
    "    phrases.decade=phrases.decade.astype(\"int32\")\n",
    "    \n",
    "    compounds.dropna(inplace=True)\n",
    "    compounds=compounds.groupby(['modifier','head','context','decade'])['count'].sum().to_frame()\n",
    "    compounds.reset_index(inplace=True)\n",
    "    compounds.decade=compounds.decade.astype(\"int32\")\n",
    "    \n",
    "    modifiers.dropna(inplace=True)\n",
    "    modifiers=modifiers.groupby(['modifier','context','decade'])['count'].sum().to_frame()\n",
    "    modifiers.reset_index(inplace=True)\n",
    "    modifiers.decade=modifiers.decade.astype(\"int32\")\n",
    "    \n",
    "    heads.dropna(inplace=True)\n",
    "    heads=heads.groupby(['head','context','decade'])['count'].sum().to_frame()\n",
    "    heads.reset_index(inplace=True)\n",
    "    heads.decade=heads.decade.astype(\"int32\")\n",
    "    return compounds,modifiers,heads,phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df, context_type=\"independant_word\",num_cores = 70):\n",
    "    num_partitions = num_cores\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    print(\"Done splitting the datasets\")\n",
    "    pool = Pool(num_cores)\n",
    "\n",
    "    cur_time=time.time()\n",
    "    print(\"Starting parallelizing\")\n",
    "    if context_type==\"dependant\":\n",
    "\n",
    "        results=pool.map_async(cdsm_reducer,df_split)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        results=results.get()\n",
    "\n",
    "        \n",
    "        print(\"Done parallelizing\")\n",
    "        print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "        compound_list = [ result[0] for result in results]\n",
    "        compounds=pd.concat(compound_list,ignore_index=True)\n",
    "        compounds=compounds.groupby(['modifier','head','context','decade'])['count'].sum().to_frame()\n",
    "        compounds.reset_index(inplace=True)\n",
    "        \n",
    "        if not isfile(\"/data/dharp/compounding/datasets/compounds_test.csv\"):\n",
    "            compounds.to_csv(\"/data/dharp/compounding/datasets/compounds_test.csv\",sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            compounds.to_csv(\"/data/dharp/compounding/datasets/compounds_test.csv\", mode='a',sep=\"\\t\", header=False,index=False)\n",
    "        \n",
    "        \n",
    "        modifier_list = [ result[1] for result in results]\n",
    "        modifiers=pd.concat(modifier_list,ignore_index=True)\n",
    "        modifiers=modifiers.groupby(['modifier','context','decade'])['count'].sum().to_frame()\n",
    "        modifiers.reset_index(inplace=True)\n",
    "\n",
    "        if not isfile(\"/data/dharp/compounding/datasets/modifiers_test.csv\"):\n",
    "            modifiers.to_csv(\"/data/dharp/compounding/datasets/modifiers_test.csv\",sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            modifiers.to_csv(\"/data/dharp/compounding/datasets/modifiers_test.csv\", mode='a', header=False,index=False)\n",
    "        \n",
    "        head_list = [ result[2] for result in results]\n",
    "        heads=pd.concat(head_list,ignore_index=True)\n",
    "        heads=heads.groupby(['head','context','decade'])['count'].sum().to_frame()\n",
    "        heads.reset_index(inplace=True)\n",
    "\n",
    "        if not isfile(\"/data/dharp/compounding/datasets/heads_test.csv\"):\n",
    "            heads.to_csv(\"/data/dharp/compounding/datasets/heads_test.csv\",sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            heads.to_csv(\"/data/dharp/compounding/datasets/heads_test.csv\", mode='a', header=False,index=False)\n",
    "            \n",
    "        phrase_list = [ result[3] for result in results]\n",
    "        phrases=pd.concat(phrase_list,ignore_index=True)\n",
    "        phrases=phrases.groupby(['modifier','head','context','decade'])['count'].sum().to_frame()\n",
    "        phrases.reset_index(inplace=True)\n",
    "        \n",
    "        if not isfile(\"/data/dharp/compounding/datasets/phrases_test.csv\"):\n",
    "            phrases.to_csv(\"/data/dharp/compounding/datasets/phrases_test.csv\",sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            phrases.to_csv(\"/data/dharp/compounding/datasets/phrases_test.csv\", mode='a', header=False,index=False)\n",
    "\n",
    "    elif context_type==\"independant_word\":\n",
    "        words_list=[]\n",
    "        results=pool.map_async(cdsm_word_reducer,df_split)\n",
    "  \n",
    "        \n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        print(\"Done parallelizing\")\n",
    "        print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "        words_list=results.get()\n",
    "        words = pd.concat(words_list,ignore_index=True,sort=False)\n",
    "        words=words.groupby(['word','context','decade'])['count'].sum().to_frame()\n",
    "        words.reset_index(inplace=True)\n",
    "        print(words.shape)\n",
    "                \n",
    "        if not isfile(\"/data/dharp/compounding/datasets/words_test.csv\"):\n",
    "            words.to_csv(\"/data/dharp/compounding/datasets/words_test.csv\",sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            words.to_csv(\"/data/dharp/compounding/datasets/words_test.csv\", mode='a',sep=\"\\t\", header=False,index=False)\n",
    "        \n",
    "    print(\"Done concatenations \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/dharp/compounding/datasets/entire_df_8_.h5 is read in\n"
     ]
    }
   ],
   "source": [
    "num_cores=24\n",
    "tmp_df=pd.read_hdf(files[0], key='df')\n",
    "print(files[0],'is read in')\n",
    "tmp_df_klein=tmp_df.head(10_000_000)\n",
    "df_split = np.array_split(tmp_df_klein, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split num: 1\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 28 secs\n",
      "Done concatenations \n",
      "\n",
      "Split num: 2\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 27 secs\n",
      "Done concatenations \n",
      "\n",
      "Split num: 3\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 30 secs\n",
      "Done concatenations \n",
      "\n",
      "Split num: 4\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 28 secs\n",
      "Done concatenations \n",
      "\n",
      "Split num: 5\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 29 secs\n",
      "Done concatenations \n",
      "\n",
      "Done writing to files\n"
     ]
    }
   ],
   "source": [
    "for num,df in enumerate(df_split):\n",
    "    print(\"Split num:\",num+1)\n",
    "    parallelize_dataframe(df,context_type=\"dependant\",num_cores=num_cores)\n",
    "print(\"Done writing to files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
