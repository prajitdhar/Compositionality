{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join,getsize\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "import pickle as pkl\n",
    "\n",
    "import warnings\n",
    "#warnings.simplefilter(action='ignore', category=ResourceWarning)\n",
    "np.random.seed(seed=1991)\n",
    "import tables\n",
    "\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_df=pd.read_pickle('/data/dharp/compounding/pkl_datasets/wo.pkl')\n",
    "trial_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_trial_phrase,new_trial_compounds,new_trial_modifier,new_trial_head=cdsm_reducer(trial_df.head(10_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modifier</th>\n",
       "      <th>context</th>\n",
       "      <th>year</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>1962</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>1969</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>1974</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>1977</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>1983</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>1984</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>1987</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>1991</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>1992</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>1993</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>1994</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>1995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>1996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>1997</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>1998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>1999</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>2002</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>2006</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>orient_noun</td>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>welt_noun</td>\n",
       "      <td>1962</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>welt_noun</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>welt_noun</td>\n",
       "      <td>1969</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>die_noun</td>\n",
       "      <td>welt_noun</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>abbey_noun</td>\n",
       "      <td>1864</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>abbey_noun</td>\n",
       "      <td>1865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>abbey_noun</td>\n",
       "      <td>1867</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>abbey_noun</td>\n",
       "      <td>1869</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>abbey_noun</td>\n",
       "      <td>1870</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>abbey_noun</td>\n",
       "      <td>1871</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>abbey_noun</td>\n",
       "      <td>1875</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>abbey_noun</td>\n",
       "      <td>1879</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>abbey_noun</td>\n",
       "      <td>1881</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>abbey_noun</td>\n",
       "      <td>1901</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>abbey_noun</td>\n",
       "      <td>1908</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>abbey_noun</td>\n",
       "      <td>1925</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>abbey_noun</td>\n",
       "      <td>1969</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>abbey_noun</td>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>be_verb</td>\n",
       "      <td>1858</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>be_verb</td>\n",
       "      <td>1860</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>be_verb</td>\n",
       "      <td>1864</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>be_verb</td>\n",
       "      <td>1865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>be_verb</td>\n",
       "      <td>1867</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>be_verb</td>\n",
       "      <td>1869</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>be_verb</td>\n",
       "      <td>1870</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>be_verb</td>\n",
       "      <td>1871</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>be_verb</td>\n",
       "      <td>1875</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>be_verb</td>\n",
       "      <td>1879</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>be_verb</td>\n",
       "      <td>1881</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>be_verb</td>\n",
       "      <td>1901</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>be_verb</td>\n",
       "      <td>1908</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>be_verb</td>\n",
       "      <td>1925</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>be_verb</td>\n",
       "      <td>1969</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>woburn_noun</td>\n",
       "      <td>be_verb</td>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>387 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        modifier      context  year  count\n",
       "0       die_noun  orient_noun  1962      1\n",
       "1       die_noun  orient_noun  1968      1\n",
       "2       die_noun  orient_noun  1969      2\n",
       "3       die_noun  orient_noun  1970      1\n",
       "4       die_noun  orient_noun  1974      1\n",
       "5       die_noun  orient_noun  1977      1\n",
       "6       die_noun  orient_noun  1983      1\n",
       "7       die_noun  orient_noun  1984      1\n",
       "8       die_noun  orient_noun  1987      4\n",
       "9       die_noun  orient_noun  1990      1\n",
       "10      die_noun  orient_noun  1991      2\n",
       "11      die_noun  orient_noun  1992      2\n",
       "12      die_noun  orient_noun  1993      1\n",
       "13      die_noun  orient_noun  1994      5\n",
       "14      die_noun  orient_noun  1995      1\n",
       "15      die_noun  orient_noun  1996      1\n",
       "16      die_noun  orient_noun  1997      2\n",
       "17      die_noun  orient_noun  1998      1\n",
       "18      die_noun  orient_noun  1999      2\n",
       "19      die_noun  orient_noun  2000      1\n",
       "20      die_noun  orient_noun  2001      1\n",
       "21      die_noun  orient_noun  2002      2\n",
       "22      die_noun  orient_noun  2003      2\n",
       "23      die_noun  orient_noun  2005      1\n",
       "24      die_noun  orient_noun  2006      6\n",
       "25      die_noun  orient_noun  2007      2\n",
       "26      die_noun    welt_noun  1962      1\n",
       "27      die_noun    welt_noun  1968      1\n",
       "28      die_noun    welt_noun  1969      2\n",
       "29      die_noun    welt_noun  1970      1\n",
       "..           ...          ...   ...    ...\n",
       "357  woburn_noun   abbey_noun  1864      3\n",
       "358  woburn_noun   abbey_noun  1865      1\n",
       "359  woburn_noun   abbey_noun  1867      2\n",
       "360  woburn_noun   abbey_noun  1869      1\n",
       "361  woburn_noun   abbey_noun  1870      3\n",
       "362  woburn_noun   abbey_noun  1871      1\n",
       "363  woburn_noun   abbey_noun  1875      7\n",
       "364  woburn_noun   abbey_noun  1879      1\n",
       "365  woburn_noun   abbey_noun  1881      5\n",
       "366  woburn_noun   abbey_noun  1901      1\n",
       "367  woburn_noun   abbey_noun  1908      2\n",
       "368  woburn_noun   abbey_noun  1925      1\n",
       "369  woburn_noun   abbey_noun  1969      1\n",
       "370  woburn_noun   abbey_noun  2003      1\n",
       "371  woburn_noun      be_verb  1858      9\n",
       "372  woburn_noun      be_verb  1860      3\n",
       "373  woburn_noun      be_verb  1864      3\n",
       "374  woburn_noun      be_verb  1865      1\n",
       "375  woburn_noun      be_verb  1867      2\n",
       "376  woburn_noun      be_verb  1869      1\n",
       "377  woburn_noun      be_verb  1870      3\n",
       "378  woburn_noun      be_verb  1871      1\n",
       "379  woburn_noun      be_verb  1875      7\n",
       "380  woburn_noun      be_verb  1879      1\n",
       "381  woburn_noun      be_verb  1881      5\n",
       "382  woburn_noun      be_verb  1901      1\n",
       "383  woburn_noun      be_verb  1908      2\n",
       "384  woburn_noun      be_verb  1925      1\n",
       "385  woburn_noun      be_verb  1969      1\n",
       "386  woburn_noun      be_verb  2003      1\n",
       "\n",
       "[387 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_trial_compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "br_to_us=pd.read_excel(\"Book.xlsx\",header=1)\n",
    "br_to_us_dict=dict(zip(br_to_us.UK.tolist(),br_to_us.US.tolist()))\n",
    "\n",
    "contextwords=pkl.load( open( \"contexts.pkl\", \"rb\" ) )\n",
    "\n",
    "\n",
    "\n",
    "spelling_replacement={'context':br_to_us_dict,'modifier':br_to_us_dict,'head':br_to_us_dict,'word':br_to_us_dict}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "any_word=r'.+_.+'\n",
    "any_noun=r'.+_noun'\n",
    "proper_noun=r'[a-z.-]+_noun'\n",
    "space=r'\\s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_maker(x, y):\n",
    "    #print(lemmatizer(x,y)[0])\n",
    "    return lemmatizer(x,y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relemjoin(df,col_name):\n",
    "    new_col=col_name.split('_')[0]\n",
    "    new_col_pos=new_col[0]+\"_pos\"\n",
    "    df[new_col]=df[col_name].str.split('_', 1).str[0]\n",
    "    df[new_col_pos]=\"noun\"\n",
    "    df[new_col]=np.vectorize(lemma_maker)(df[new_col], df[new_col_pos])\n",
    "    df.replace(spelling_replacement,inplace=True)\n",
    "    df[new_col]=df[new_col]+\"_noun\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntactic_reducer(df,align,level=None):\n",
    "    if len(df) == 0:\n",
    "        print(\"Am here\")\n",
    "        return df\n",
    "    if align==\"right\":\n",
    "        if level==\"word\":\n",
    "            #t1=time.time()\n",
    "            df=df.loc[df.fivegram_pos.str.match(r\"^\"+any_noun+space+(any_word+space)*3+any_word+\"$\")]\n",
    "            if len(df) == 0:\n",
    "                return df\n",
    "            \n",
    "            df['word_pos'],df['r1_pos'],df['r2_pos'],df['r3_pos'],_=df['fivegram_pos'].str.split(space).str\n",
    "            #df=df.query('word_pos == @word_list')\n",
    "            df=relemjoin(df,'word_pos')\n",
    "            df=pd.melt(df,id_vars=['word','year','count'],value_vars=['r1_pos','r2_pos','r3_pos'])\n",
    "            #print(time.time()-t1)\n",
    "            return df\n",
    "        else:\n",
    "            #phrases=df.loc[df.fivegram_pos.str.match(r'^[-a-z]+_noun\\s+[-a-z]+_noun\\s+[-a-z]+_.+\\s+[-a-z]+_.+\\s+[-a-z]+_.+$')]\n",
    "            phrases=df.loc[df.fivegram_pos.str.match(r'^'+(any_noun+space)*2+(any_word+space)*2+any_word+'$')]\n",
    "\n",
    "            #cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^[-a-z]+_noun\\s+[-a-z]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_.+\\s+[a-z-]+_.+$')]\n",
    "            cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^'+(any_noun+space)*3+(any_word+space)+any_word+'$')]\n",
    "            \n",
    "            cdsm=cdsm.loc[cdsm.fivegram_pos.str.match(r'^'+(proper_noun+space)*2+(any_word+space)*2+any_word+'$')]\n",
    "\n",
    "            try:\n",
    "                phrases['modifier_pos'],phrases['head_pos'],phrases['r1_pos'],phrases['r2_pos'],phrases['r3_pos']=phrases['fivegram_pos'].str.split(space).str\n",
    "                cdsm['modifier_pos'],cdsm['head_pos'],cdsm['r1_pos'],cdsm['r2_pos'],cdsm['r3_pos']=cdsm['fivegram_pos'].str.split(space).str\n",
    "            except ValueError:\n",
    "                phrases=pd.DataFrame()\n",
    "                compounds=pd.DataFrame()\n",
    "                modifiers=pd.DataFrame()\n",
    "                heads=pd.DataFrame()\n",
    "                return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            phrases=relemjoin(phrases,'modifier_pos')\n",
    "            phrases=relemjoin(phrases,'head_pos')\n",
    "            cdsm=relemjoin(cdsm,'modifier_pos')\n",
    "            cdsm=relemjoin(cdsm,'head_pos')\n",
    "            \n",
    "            phrases=pd.melt(phrases,id_vars=['modifier','head','year','count'],value_vars=['r1_pos','r2_pos','r3_pos'])\n",
    "            compounds=pd.melt(cdsm,id_vars=['modifier','head','year','count'],value_vars=['r1_pos','r2_pos','r3_pos'])\n",
    "            modifiers=pd.melt(cdsm,id_vars=['modifier','year','count'],value_vars=['head','r1_pos','r2_pos'])\n",
    "            heads=pd.melt(cdsm,id_vars=['head','year','count'],value_vars=['modifier','r1_pos','r2_pos','r3_pos'])\n",
    "\n",
    "            return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            \n",
    "    elif align==\"mid1\":\n",
    "        if level==\"word\":\n",
    "            #df=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+$')]\n",
    "            #df=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+$')]\n",
    "            df=df.loc[df.fivegram_pos.str.match(r\"^\"+any_word+space+any_noun+space+(any_word+space)*2+any_word+\"$\")]\n",
    "\n",
    "            if len(df) == 0:\n",
    "                return df\n",
    "            \n",
    "            df['l1_pos'],df['word_pos'],df['r1_pos'],df['r2_pos'],df['r3_pos']=df['fivegram_pos'].str.split(space).str\n",
    "            df=relemjoin(df,'word_pos')\n",
    "            df=pd.melt(df,id_vars=['word','year','count'],value_vars=['l1_pos','r1_pos','r2_pos','r3_pos'])\n",
    "            return df\n",
    "        else:\n",
    "            #phrases=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_.+\\s+[a-z-]+_.+$')]\n",
    "            phrases=df.loc[df.fivegram_pos.str.match(r'^'+any_word+space+(any_noun+space)*2+any_word+space+any_word+'$')]\n",
    "            #cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_.+$')]\n",
    "\n",
    "            \n",
    "            cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^'+(any_noun+space)*4+any_word+'$')]\n",
    "            \n",
    "            cdsm=cdsm.loc[cdsm.fivegram_pos.str.match(r'^'+any_word+space+(proper_noun+space)*2+any_word+space+any_word+'$')]\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                phrases['l1_pos'],phrases['modifier_pos'],phrases['head_pos'],phrases['r1_pos'],phrases['r2_pos']=phrases['fivegram_pos'].str.split(space).str\n",
    "                cdsm['l1_pos'],cdsm['modifier_pos'],cdsm['head_pos'],cdsm['r1_pos'],cdsm['r2_pos']=cdsm['fivegram_pos'].str.split(space).str\n",
    "            except ValueError:\n",
    "                phrases=pd.DataFrame()\n",
    "                compounds=pd.DataFrame()\n",
    "                modifiers=pd.DataFrame()\n",
    "                heads=pd.DataFrame()\n",
    "                return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            phrases=relemjoin(phrases,'modifier_pos')\n",
    "            phrases=relemjoin(phrases,'head_pos')\n",
    "            cdsm=relemjoin(cdsm,'modifier_pos')\n",
    "            cdsm=relemjoin(cdsm,'head_pos')\n",
    "            \n",
    "            phrases=pd.melt(phrases,id_vars=['modifier','head','year','count'],value_vars=['l1_pos','r1_pos','r2_pos'])\n",
    "            compounds=pd.melt(cdsm,id_vars=['modifier','head','year','count'],value_vars=['l1_pos','r1_pos','r2_pos'])\n",
    "            modifiers=pd.melt(cdsm,id_vars=['modifier','year','count'],value_vars=['head','l1_pos','r1_pos','r2_pos'])\n",
    "            heads=pd.melt(cdsm,id_vars=['head','year','count'],value_vars=['modifier','l1_pos','r1_pos','r2_pos'])\n",
    "            return phrases,compounds,modifiers,heads\n",
    "    \n",
    "            \n",
    "    elif align==\"mid2\":\n",
    "        if level==\"word\":\n",
    "            #df=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_.+\\s+[a-z-]+_.+$')]\n",
    "            df=df.loc[df.fivegram_pos.str.match(r'^'+(any_word+space)*2+any_noun+space+any_word+space+any_word+'$')]\n",
    "            if len(df) == 0:\n",
    "                return df\n",
    "            \n",
    "            df['l1_pos'],df['l2_pos'],df['word_pos'],df['r1_pos'],df['r2_pos']=df['fivegram_pos'].str.split(space).str\n",
    "            df=relemjoin(df,'word_pos')\n",
    "            df=pd.melt(df,id_vars=['word','year','count'],value_vars=['l1_pos','l2_pos','r1_pos','r2_pos'])\n",
    "            return df\n",
    "        else:\n",
    "            \n",
    "            #phrases=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_.+$')]\n",
    "            \n",
    "            phrases=df.loc[df.fivegram_pos.str.match(r'^'+(any_word+space)*2+(any_noun+space)*2+any_word+'$')]\n",
    "            #cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_noun$')]\n",
    "\n",
    "            cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^'+any_word+space+(any_noun+space)*3+any_word+'$')]\n",
    "            cdsm=cdsm.loc[cdsm.fivegram_pos.str.match(r'^'+(any_word+space)*2+(proper_noun+space)*2+any_word+'$')]\n",
    "            try:\n",
    "                phrases['l1_pos'],phrases['l2_pos'],phrases['modifier_pos'],phrases['head_pos'],phrases['r1_pos']=phrases['fivegram_pos'].str.split(space).str\n",
    "                cdsm['l1_pos'],cdsm['l2_pos'],cdsm['modifier_pos'],cdsm['head_pos'],cdsm['r1_pos']=cdsm['fivegram_pos'].str.split(space).str\n",
    "            except ValueError:\n",
    "                phrases=pd.DataFrame()\n",
    "                compounds=pd.DataFrame()\n",
    "                modifiers=pd.DataFrame()\n",
    "                heads=pd.DataFrame()\n",
    "                return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            phrases=relemjoin(phrases,'modifier_pos')\n",
    "            phrases=relemjoin(phrases,'head_pos')\n",
    "            cdsm=relemjoin(cdsm,'modifier_pos')\n",
    "            cdsm=relemjoin(cdsm,'head_pos')\n",
    "            \n",
    "            phrases=pd.melt(phrases,id_vars=['modifier','head','year','count'],value_vars=['l1_pos','l2_pos','r1_pos'])\n",
    "            compounds=pd.melt(cdsm,id_vars=['modifier','head','year','count'],value_vars=['l1_pos','l2_pos','r1_pos'])\n",
    "            modifiers=pd.melt(cdsm,id_vars=['modifier','year','count'],value_vars=['head','l1_pos','l2_pos','r1_pos'])\n",
    "            heads=pd.melt(cdsm,id_vars=['head','year','count'],value_vars=['modifier','l1_pos','l2_pos','r1_pos'])\n",
    "            return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            \n",
    "    elif align==\"mid3\":\n",
    "        #df=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_.+$')]\n",
    "        df=df.loc[df.fivegram_pos.str.match(r'^'+(any_word+space)*3+any_noun+space+any_word+'$')]\n",
    "        if len(df)==0:\n",
    "            return df\n",
    "\n",
    "        df['l1_pos'],df['l2_pos'],df['word_pos'],df['r1_pos'],df['r2_pos']=df['fivegram_pos'].str.split(space).str\n",
    "        df=relemjoin(df,'word_pos')\n",
    "        df=pd.melt(df,id_vars=['word','year','count'],value_vars=['l1_pos','l2_pos','r1_pos','r2_pos'])\n",
    "        return df\n",
    "        \n",
    "    elif align==\"left\":\n",
    "        \n",
    "        if level==\"word\":\n",
    "            #df=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun$')]\n",
    "            \n",
    "            df=df.loc[df.fivegram_pos.str.match(r'^'+(any_word+space)*4+any_noun+'$')]\n",
    "            if len(df) == 0:\n",
    "                return df\n",
    "            _,df['l1_pos'],df['l2_pos'],df['l3_pos'],df['word_pos']=df['fivegram_pos'].str.split(space).str\n",
    "            df=relemjoin(df,'word_pos')\n",
    "            df=pd.melt(df,id_vars=['word','year','count'],value_vars=['l1_pos','l2_pos','l3_pos'])\n",
    "            return df\n",
    "        else:\n",
    "            #phrases=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_noun$')]\n",
    "            \n",
    "            phrases=df.loc[df.fivegram_pos.str.match(r'^'+(any_word+space)*3+any_noun+space+any_noun+'$')]\n",
    "            #cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_noun$')]\n",
    "            \n",
    "            cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^'+(any_word+space)*2+(any_noun+space)*2+any_noun+'$')]\n",
    "            cdsm=cdsm.loc[cdsm.fivegram_pos.str.match(r'^'+(any_word+space)*3+proper_noun+space+proper_noun+'$')]\n",
    "            \n",
    "            try:\n",
    "                phrases['l1_pos'],phrases['l2_pos'],phrases['l3_pos'],phrases['modifier_pos'],phrases['head_pos']=phrases['fivegram_pos'].str.split(space).str\n",
    "                cdsm['l1_pos'],cdsm['l2_pos'],cdsm['l3_pos'],cdsm['modifier_pos'],cdsm['head_pos']=cdsm['fivegram_pos'].str.split(space).str\n",
    "            except ValueError:\n",
    "                phrases=pd.DataFrame()\n",
    "                compounds=pd.DataFrame()\n",
    "                modifiers=pd.DataFrame()\n",
    "                heads=pd.DataFrame()\n",
    "                return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            phrases=relemjoin(phrases,'modifier_pos')\n",
    "            phrases=relemjoin(phrases,'head_pos')\n",
    "            cdsm=relemjoin(cdsm,'modifier_pos')\n",
    "            cdsm=relemjoin(cdsm,'head_pos')\n",
    "            \n",
    "            phrases=pd.melt(phrases,id_vars=['modifier','head','year','count'],value_vars=['l1_pos','l2_pos','l3_pos'])\n",
    "            compounds=pd.melt(cdsm,id_vars=['modifier','head','year','count'],value_vars=['l1_pos','l2_pos','l3_pos'])\n",
    "            modifiers=pd.melt(cdsm,id_vars=['modifier','year','count'],value_vars=['head','l1_pos','l2_pos','l3_pos'])\n",
    "            heads=pd.melt(cdsm,id_vars=['head','year','count'],value_vars=['modifier','l1_pos','l2_pos','l3_pos'])\n",
    "            return phrases,compounds,modifiers,heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_reducer(df):\n",
    "    if len(df)==0:\n",
    "        return df\n",
    "    df[\"variable\"]=df[\"variable\"].str.replace(r\"_pos\",\"\")\n",
    "    df[\"context\"],df[\"context_pos\"]=df['value'].str.split('_', 1).str\n",
    "    df=df.loc[df.context_pos.isin([\"noun\",\"adj\",\"adv\",\"verb\"])]\n",
    "    #df.replace(adv_replacement,inplace=True)\n",
    "    #df['context_pos']=df['context_pos'].str[0]\n",
    "    if len(df)==0:\n",
    "        return df\n",
    "    df['context']=np.vectorize(lemma_maker)(df['context'], df['context_pos'])\n",
    "    df.replace(spelling_replacement,inplace=True)\n",
    "    df['context']=df['context']+\"_\"+df['context_pos']\n",
    "    df.query('context in @contextwords',inplace=True)\n",
    "    #df.reset_index(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdsm_word_reducer(df):\n",
    "    rightgram=syntactic_reducer(df,align=\"right\",level=\"word\")\n",
    "    rightgram=context_reducer(rightgram)\n",
    "    \n",
    "    mid1gram=syntactic_reducer(df,align=\"mid1\",level=\"word\")\n",
    "    mid1gram=context_reducer(mid1gram)\n",
    "    \n",
    "    mid2gram=syntactic_reducer(df,align=\"mid2\",level=\"word\")\n",
    "    mid2gram=context_reducer(mid2gram)\n",
    "    \n",
    "    mid3gram=syntactic_reducer(df,align=\"mid3\",level=\"word\")\n",
    "    mid3gram=context_reducer(mid3gram)\n",
    "    \n",
    "    leftgram=syntactic_reducer(df,align=\"left\",level=\"word\")\n",
    "    leftgram=context_reducer(leftgram)    \n",
    "    \n",
    "    words_df=pd.concat([rightgram,mid1gram,mid2gram,mid3gram,leftgram],ignore_index=True,sort=False)\n",
    "    words_df.dropna(inplace=True)\n",
    "    #words_df=words_df.query('word in @word_list')\n",
    "    words_df=words_df.groupby(['word','context','year'])['count'].sum().to_frame()\n",
    "    words_df.reset_index(inplace=True)\n",
    "    words_df.year=words_df.year.astype(\"int32\")\n",
    "    #print(words_df.shape)\n",
    "    return words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdsm_reducer(df):\n",
    "    phrase_rightgram,compound_rightgram,modifier_rightgram,head_rightgram=syntactic_reducer(df,align=\"right\")\n",
    "    phrase_rightgram=context_reducer(phrase_rightgram)\n",
    "    compound_rightgram=context_reducer(compound_rightgram)\n",
    "    modifier_rightgram=context_reducer(modifier_rightgram)\n",
    "    head_rightgram=context_reducer(head_rightgram)\n",
    "\n",
    "\n",
    "    phrase_mid1gram,compound_mid1gram,modifier_mid1gram,head_mid1gram=syntactic_reducer(df,align=\"mid1\")\n",
    "    phrase_mid1gram=context_reducer(phrase_mid1gram)\n",
    "    compound_mid1gram=context_reducer(compound_mid1gram)\n",
    "    modifier_mid1gram=context_reducer(modifier_mid1gram)\n",
    "    head_mid1gram=context_reducer(head_mid1gram)\n",
    " \n",
    "\n",
    "    phrase_mid2gram,compound_mid2gram,modifier_mid2gram,head_mid2gram=syntactic_reducer(df,align=\"mid2\")\n",
    "    phrase_mid2gram=context_reducer(phrase_mid2gram)\n",
    "    compound_mid2gram=context_reducer(compound_mid2gram)\n",
    "    modifier_mid2gram=context_reducer(modifier_mid2gram)\n",
    "    head_mid2gram=context_reducer(head_mid2gram)\n",
    "    \n",
    "    phrase_leftgram,compound_leftgram,modifier_leftgram,head_leftgram=syntactic_reducer(df,align=\"left\")\n",
    "    phrase_leftgram=context_reducer(phrase_leftgram)\n",
    "    compound_leftgram=context_reducer(compound_leftgram)\n",
    "    modifier_leftgram=context_reducer(modifier_leftgram)\n",
    "    head_leftgram=context_reducer(head_leftgram)\n",
    "    \n",
    "    \n",
    "    phrases=pd.concat([phrase_rightgram,phrase_mid1gram,phrase_mid2gram,phrase_leftgram],ignore_index=True,sort=False)\n",
    "    compounds=pd.concat([compound_rightgram,compound_mid1gram,compound_mid2gram,compound_leftgram],ignore_index=True,sort=False)\n",
    "    modifiers=pd.concat([modifier_rightgram,modifier_mid1gram,modifier_mid2gram,modifier_leftgram],ignore_index=True,sort=False)\n",
    "    heads=pd.concat([head_rightgram,head_mid1gram,head_mid2gram,head_leftgram],ignore_index=True,sort=False)\n",
    "\n",
    "    \n",
    "    phrases.dropna(inplace=True)\n",
    "    phrases=phrases.groupby(['modifier','head','context','year'])['count'].sum().to_frame()\n",
    "    phrases.reset_index(inplace=True)\n",
    "    phrases.year=phrases.year.astype(\"int32\")\n",
    "    \n",
    "    compounds.dropna(inplace=True)\n",
    "    compounds=compounds.groupby(['modifier','head','context','year'])['count'].sum().to_frame()\n",
    "    compounds.reset_index(inplace=True)\n",
    "    compounds.year=compounds.year.astype(\"int32\")\n",
    "    \n",
    "    modifiers.dropna(inplace=True)\n",
    "    modifiers=modifiers.groupby(['modifier','context','year'])['count'].sum().to_frame()\n",
    "    modifiers.reset_index(inplace=True)\n",
    "    modifiers.year=modifiers.year.astype(\"int32\")\n",
    "    \n",
    "    heads.dropna(inplace=True)\n",
    "    heads=heads.groupby(['head','context','year'])['count'].sum().to_frame()\n",
    "    heads.reset_index(inplace=True)\n",
    "    heads.year=heads.year.astype(\"int32\")\n",
    "    return compounds,modifiers,heads,phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df, context_type=\"independant_word\",num_cores = 70):\n",
    "    num_partitions = num_cores\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    print(\"Done splitting the datasets\")\n",
    "    pool = Pool(num_cores)\n",
    "\n",
    "    cur_time=time.time()\n",
    "    print(\"Starting parallelizing\")\n",
    "    if context_type==\"dependant\":\n",
    "\n",
    "        results=pool.map_async(cdsm_reducer,df_split)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        results=results.get()\n",
    "\n",
    "        \n",
    "        print(\"Done parallelizing\")\n",
    "        print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "        compound_list = [ result[0] for result in results]\n",
    "        compounds=pd.concat(compound_list,ignore_index=True)\n",
    "        compounds=compounds.groupby(['modifier','head','context','decade'])['count'].sum().to_frame()\n",
    "        compounds.reset_index(inplace=True)\n",
    "        \n",
    "        if not isfile(\"/data/dharp/compounding/datasets/compounds_test.csv\"):\n",
    "            compounds.to_csv(\"/data/dharp/compounding/datasets/compounds_test.csv\",sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            compounds.to_csv(\"/data/dharp/compounding/datasets/compounds_test.csv\", mode='a',sep=\"\\t\", header=False,index=False)\n",
    "        \n",
    "        \n",
    "        modifier_list = [ result[1] for result in results]\n",
    "        modifiers=pd.concat(modifier_list,ignore_index=True)\n",
    "        modifiers=modifiers.groupby(['modifier','context','decade'])['count'].sum().to_frame()\n",
    "        modifiers.reset_index(inplace=True)\n",
    "\n",
    "        if not isfile(\"/data/dharp/compounding/datasets/modifiers_test.csv\"):\n",
    "            modifiers.to_csv(\"/data/dharp/compounding/datasets/modifiers_test.csv\",sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            modifiers.to_csv(\"/data/dharp/compounding/datasets/modifiers_test.csv\", mode='a', header=False,index=False)\n",
    "        \n",
    "        head_list = [ result[2] for result in results]\n",
    "        heads=pd.concat(head_list,ignore_index=True)\n",
    "        heads=heads.groupby(['head','context','decade'])['count'].sum().to_frame()\n",
    "        heads.reset_index(inplace=True)\n",
    "\n",
    "        if not isfile(\"/data/dharp/compounding/datasets/heads_test.csv\"):\n",
    "            heads.to_csv(\"/data/dharp/compounding/datasets/heads_test.csv\",sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            heads.to_csv(\"/data/dharp/compounding/datasets/heads_test.csv\", mode='a', header=False,index=False)\n",
    "            \n",
    "        phrase_list = [ result[3] for result in results]\n",
    "        phrases=pd.concat(phrase_list,ignore_index=True)\n",
    "        phrases=phrases.groupby(['modifier','head','context','decade'])['count'].sum().to_frame()\n",
    "        phrases.reset_index(inplace=True)\n",
    "        \n",
    "        if not isfile(\"/data/dharp/compounding/datasets/phrases_test.csv\"):\n",
    "            phrases.to_csv(\"/data/dharp/compounding/datasets/phrases_test.csv\",sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            phrases.to_csv(\"/data/dharp/compounding/datasets/phrases_test.csv\", mode='a', header=False,index=False)\n",
    "\n",
    "    elif context_type==\"independant_word\":\n",
    "        words_list=[]\n",
    "        results=pool.map_async(cdsm_word_reducer,df_split)\n",
    "  \n",
    "        \n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        print(\"Done parallelizing\")\n",
    "        print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "        words_list=results.get()\n",
    "        words = pd.concat(words_list,ignore_index=True,sort=False)\n",
    "        words=words.groupby(['word','context','decade'])['count'].sum().to_frame()\n",
    "        words.reset_index(inplace=True)\n",
    "        print(words.shape)\n",
    "                \n",
    "        if not isfile(\"/data/dharp/compounding/datasets/words_test.csv\"):\n",
    "            words.to_csv(\"/data/dharp/compounding/datasets/words_test.csv\",sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            words.to_csv(\"/data/dharp/compounding/datasets/words_test.csv\", mode='a',sep=\"\\t\", header=False,index=False)\n",
    "        \n",
    "    print(\"Done concatenations \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/dharp/compounding/datasets/entire_df_8_.h5 is read in\n"
     ]
    }
   ],
   "source": [
    "num_cores=24\n",
    "tmp_df=pd.read_hdf(files[0], key='df')\n",
    "print(files[0],'is read in')\n",
    "tmp_df_klein=tmp_df.head(10_000_000)\n",
    "df_split = np.array_split(tmp_df_klein, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split num: 1\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 28 secs\n",
      "Done concatenations \n",
      "\n",
      "Split num: 2\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 27 secs\n",
      "Done concatenations \n",
      "\n",
      "Split num: 3\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 30 secs\n",
      "Done concatenations \n",
      "\n",
      "Split num: 4\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 28 secs\n",
      "Done concatenations \n",
      "\n",
      "Split num: 5\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 29 secs\n",
      "Done concatenations \n",
      "\n",
      "Done writing to files\n"
     ]
    }
   ],
   "source": [
    "for num,df in enumerate(df_split):\n",
    "    print(\"Split num:\",num+1)\n",
    "    parallelize_dataframe(df,context_type=\"dependant\",num_cores=num_cores)\n",
    "print(\"Done writing to files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
